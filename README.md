# üöÄ Plateforme d'Extraction et d'Analyse de Produits Amazon

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![Airflow](https://img.shields.io/badge/Airflow-2.5.0%2B-red)
![Snowflake](https://img.shields.io/badge/Snowflake-Ready-9cf)
![License](https://img.shields.io/badge/License-MIT-green)

> Architecture distribu√©e d'extraction et d'analyse de donn√©es e-commerce optimis√©e pour Amazon, construite sur Python et Apache Airflow avec int√©gration Snowflake.

## üîç Pr√©sentation

Cette plateforme permet l'extraction et l'analyse de donn√©es produits depuis Amazon, en offrant une solution compl√®te et performante gr√¢ce √† une architecture distribu√©e et scalable.

**Capacit√©s principales:**
- Collecte multi-cat√©gories avec traitement parall√©lis√©
- Extraction exhaustive des m√©tadonn√©es produits, taxonomie et avis utilisateurs
- Syst√®mes anti-blocage avanc√©s
- Pipeline d'analyse complet via Snowflake

## üìã Table des mati√®res

- [Fonctionnalit√©s](#-fonctionnalit√©s)
- [Pr√©requis](#-pr√©requis)
- [Installation](#-installation)
- [Architecture](#-architecture)
- [Configuration](#-configuration)
- [Pipeline d'ex√©cution](#-pipeline-dex√©cution)
- [Mod√®le de donn√©es](#-mod√®le-de-donn√©es)
- [Exemples d'utilisation](#-exemples-dutilisation)
- [R√©solution des probl√®mes](#-r√©solution-des-probl√®mes)
- [S√©curit√© et conformit√©](#-s√©curit√©-et-conformit√©)
- [Performance](#-performance-et-optimisation)
- [D√©veloppements futurs](#-d√©veloppements-futurs)
- [Licence](#-licence)

## ‚ú® Fonctionnalit√©s

### üîç Extraction de donn√©es
- **Collecte multi-cat√©gories**: Syst√®me optimis√© pour extraire des donn√©es de plusieurs cat√©gories Amazon simultan√©ment
- **Extraction exhaustive**:
  - M√©tadonn√©es produits (identifiants, prix, sp√©cifications)
  - Taxonomie des cat√©gories et syst√®mes de filtrage
  - Donn√©es utilisateurs (avis, √©valuations)
- **Anti-blocage avanc√©**:
  - Gestion intelligente du throttling avec backoff exponentiel
  - Rotation d'agents utilisateurs et proxies
  - Modulation dynamique des taux de requ√™tes

### ‚öôÔ∏è Traitement et pipeline
- **Orchestration via Apache Airflow**: Gestion compl√®te du workflow avec DAGs configurables
- **Parall√©lisation optimis√©e**: Utilisation de ThreadPoolExecutor pour maximiser l'efficacit√©
- **Exportation multi-formats**: Support des formats CSV et JSON avec validation structurelle
- **Observabilit√© int√©gr√©e**: Syst√®me complet de m√©triques, logs structur√©s et rapports d'ex√©cution

### üìä Analyse de donn√©es via Snowflake
- **Pipeline ETL**: Syst√®me d'ingestion et transformation automatis√©
- **Mod√©lisation analytique**: Tables normalis√©es et d√©normalis√©es pour diff√©rents cas d'usage
- **Capacit√©s analytiques**: Segmentation produit, analyse de sentiment et autres fonctionnalit√©s

## üìã Pr√©requis

- Python 3.8+
- Apache Airflow 2.5.0+
- PostgreSQL 13+ (recommand√© pour Airflow)
- Compte Snowflake avec permissions ad√©quates
- *(Optionnel)* AWS S3 pour le stockage interm√©diaire des donn√©es extraites

### D√©pendances Python

```
apache-airflow>=2.5.0
requests>=2.28.0
beautifulsoup4>=4.11.0
pandas>=1.5.0
selenium>=4.7.0
webdriver-manager>=3.8.0
python-dotenv>=0.21.0
snowflake-connector-python>=2.7.0
```

## üîß Installation

```bash
# Clonage du r√©f√©rentiel
git clone https://github.com/zakariaeyahya/amazon.git
cd amazon-scraping

# Configuration de l'environnement virtuel
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# D√©finition du r√©pertoire home d'Airflow
export AIRFLOW_HOME=/path/to/airflow

# Initialisation de la base de donn√©es
airflow db init

# Cr√©ation de l'utilisateur administrateur
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com \
    --password your_password

# Configuration des r√©pertoires n√©cessaires
mkdir -p $AIRFLOW_HOME/{dags,logs,plugins}
```

## üèóÔ∏è Architecture

```
amazon-analytics/
‚îú‚îÄ‚îÄ airflow_dags/
‚îÇ   ‚îî‚îÄ‚îÄ amazon_scraping_dag.py
‚îú‚îÄ‚îÄ scrapers/
‚îÇ   ‚îú‚îÄ‚îÄ categories.py
‚îÇ   ‚îú‚îÄ‚îÄ amazon_details_scraper.py
‚îÇ   ‚îú‚îÄ‚îÄ commentaire.py
‚îÇ   ‚îî‚îÄ‚îÄ scraping.py
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ file_utils.py
‚îÇ   ‚îú‚îÄ‚îÄ scraping_logger.py
‚îÇ   ‚îî‚îÄ‚îÄ scraping_metrics.py
‚îú‚îÄ‚îÄ snowflake/
‚îÇ   ‚îú‚îÄ‚îÄ SQLrequet.sql
‚îÇ   ‚îî‚îÄ‚îÄ snowflakeconnection.py
‚îú‚îÄ‚îÄ data/
‚îú‚îÄ‚îÄ logs/
‚îú‚îÄ‚îÄ config/
‚îú‚îÄ‚îÄ tests/
‚îî‚îÄ‚îÄ requirements.txt
```

## ‚öôÔ∏è Configuration

### Configuration de l'extracteur

```json
{
  "extraction": {
    "categories": ["electronics", "computers", "smart-home"],
    "limites": {
      "produits_par_categorie": 100,
      "pages_par_categorie": 5
    },
    "controle_debit": {
      "requetes_par_seconde": 2,
      "taille_burst": 5,
      "delai_apres_echec": 300
    },
    "proxies": {
      "actifs": true,
      "liste": [
        "http://proxy1.example.com:8080",
        "http://proxy2.example.com:8080"
      ],
      "intervalle_rotation": 100
    },
    "agents_utilisateurs": {
      "actifs": true,
      "intervalle_rotation": 50
    }
  },
  "traitement": {
    "taille_lot": 5,
    "travailleurs_max": 2
  }
}
```

### Configuration Snowflake

```python
# snowflakeconnection.py
conn_config = {
    'user': 'SNOWFLAKE_USER',
    'password': 'SNOWFLAKE_PASSWORD',
    'account': 'SNOWFLAKE_ACCOUNT',
    'warehouse': 'COMPUTE_WH',
    'database': 'AMAZON_ANALYTICS',
    'schema': 'PUBLIC'
}
```

## üîÑ Pipeline d'ex√©cution

### Phase d'extraction
1. V√©rification pr√©liminaire - Validation de l'environnement et des d√©pendances
2. Extraction des cat√©gories - Cartographie de la taxonomie produit
3. Extraction des d√©tails produits - Collecte des m√©tadonn√©es principales
4. Extraction des avis - Collecte des donn√©es utilisateurs
5. Post-traitement - Nettoyage et normalisation
6. G√©n√©ration de rapports - M√©triques d'ex√©cution et validations

### Phase d'analyse Snowflake
1. Chargement des donn√©es - Ingestion dans les tables Snowflake
2. Transformation - Normalisation et enrichissement
3. Mod√©lisation - Cr√©ation des vues analytiques
4. Segmentation - Classification par prix et autres dimensions
5. Analyse avanc√©e - Requ√™tes analytiques et rapports

## üíæ Mod√®le de donn√©es

### Tables principales

| Table | Description |
|-------|-------------|
| `product_info1` | Donn√©es fondamentales produit |
| `product_info2` | Sp√©cifications techniques produit |
| `review` | Avis et √©valuations utilisateurs |
| `full_product_info` | Vue d√©normalis√©e int√©gr√©e |
| `classe_prix` | Segmentation par gamme de prix |

> **Note:** Le sch√©ma est con√ßu pour optimiser √† la fois les requ√™tes analytiques et les op√©rations OLTP.

## üí° Exemples d'utilisation

### Classification de produits et analyse de sentiment

```sql
SELECT
    p.asin,
    p.titre,
    p.prix,
    c.classe AS gamme_prix,
    r.sentiment,
    COUNT(r.review_id) AS nombre_avis
FROM
    full_product_info p
JOIN
    classe_prix c ON p.asin = c.asin
LEFT JOIN
    review r ON p.asin = r.asin
GROUP BY
    1, 2, 3, 4, 5
ORDER BY
    nombre_avis DESC;
```

### Transformation des sp√©cifications techniques

```sql
-- Normalisation des tailles de disque dur en GB
UPDATE product_info2
SET disque_dur =
    CASE
        WHEN disque_dur LIKE '%TB%' THEN TO_NUMBER(REGEXP_SUBSTR(disque_dur, '[0-9]+')) * 1024
        WHEN disque_dur LIKE '%GB%' THEN TO_NUMBER(REGEXP_SUBSTR(disque_dur, '[0-9]+'))
        ELSE NULL
    END;
```

## üîß R√©solution des probl√®mes

### DAGs Airflow
- V√©rifier les permissions des fichiers DAG
- Valider la syntaxe Python avec `airflow dags list` et `airflow dags show`
- Consulter les logs pour erreurs de syntaxe ou d'importation

### Limitations de taux
- Impl√©menter une strat√©gie de backoff progressif
- Augmenter l'intervalle entre requ√™tes
- Diversifier les proxies et agents utilisateurs

### Connexion Snowflake
- V√©rifier les identifiants et permissions
- S'assurer que l'adresse IP est autoris√©e
- Tester la connexion avec une requ√™te simple

## üîí S√©curit√© et conformit√©

‚ö†Ô∏è **Important:** Ce projet est con√ßu pour un usage professionnel et √©ducatif. L'utilisation doit √™tre conforme aux:
- Conditions d'utilisation d'Amazon
- Directives du fichier robots.txt
- R√©glementations en vigueur sur la protection des donn√©es

### Bonnes pratiques de s√©curit√©
- Ne jamais stocker les identifiants en clair dans le code
- Utiliser des variables d'environnement ou un gestionnaire de secrets
- Impl√©menter une politique de limitation de requ√™tes raisonnable
- Auditer r√©guli√®rement les logs d'acc√®s
- Chiffrer les donn√©es sensibles lors du stockage et du transit


## üìú Licence

Ce projet est distribu√© sous [licence MIT](LICENSE).

---

¬© 2025 - Plateforme d'Extraction et d'Analyse de Produits Amazon - Tous droits r√©serv√©s
