# ðŸš€ Plateforme d'Extraction et d'Analyse de Produits Amazon

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![Airflow](https://img.shields.io/badge/Airflow-2.5.0%2B-red)
![Snowflake](https://img.shields.io/badge/Snowflake-Ready-9cf)
![License](https://img.shields.io/badge/License-MIT-green)

> Architecture distribuÃ©e d'extraction et d'analyse de donnÃ©es e-commerce optimisÃ©e pour Amazon, construite sur Python et Apache Airflow avec intÃ©gration Snowflake.
![Capture dâ€™Ã©cran_19-5-2025_205939_](https://github.com/user-attachments/assets/62fb053a-24ce-4501-ae44-25522aca7262)

## ðŸ” PrÃ©sentation

Cette plateforme permet l'extraction et l'analyse de donnÃ©es produits depuis Amazon, en offrant une solution complÃ¨te et performante grÃ¢ce Ã  une architecture distribuÃ©e et scalable.

**CapacitÃ©s principales:**
- Collecte multi-catÃ©gories avec traitement parallÃ©lisÃ©
- Extraction exhaustive des mÃ©tadonnÃ©es produits, taxonomie et avis utilisateurs
- SystÃ¨mes anti-blocage avancÃ©s
- Pipeline d'analyse complet via Snowflake

## ðŸ“‹ Table des matiÃ¨res

- [FonctionnalitÃ©s](#-fonctionnalitÃ©s)
- [PrÃ©requis](#-prÃ©requis)
- [Installation](#-installation)
- [Architecture](#-architecture)
- [Configuration](#-configuration)
- [Pipeline d'exÃ©cution](#-pipeline-dexÃ©cution)
- [ModÃ¨le de donnÃ©es](#-modÃ¨le-de-donnÃ©es)
- [Exemples d'utilisation](#-exemples-dutilisation)
- [RÃ©solution des problÃ¨mes](#-rÃ©solution-des-problÃ¨mes)
- [SÃ©curitÃ© et conformitÃ©](#-sÃ©curitÃ©-et-conformitÃ©)
- [Performance](#-performance-et-optimisation)
- [DÃ©veloppements futurs](#-dÃ©veloppements-futurs)
- [Licence](#-licence)

## âœ¨ FonctionnalitÃ©s

### ðŸ” Extraction de donnÃ©es
- **Collecte multi-catÃ©gories**: SystÃ¨me optimisÃ© pour extraire des donnÃ©es de plusieurs catÃ©gories Amazon simultanÃ©ment
- **Extraction exhaustive**:
  - MÃ©tadonnÃ©es produits (identifiants, prix, spÃ©cifications)
  - Taxonomie des catÃ©gories et systÃ¨mes de filtrage
  - DonnÃ©es utilisateurs (avis, Ã©valuations)
- **Anti-blocage avancÃ©**:
  - Gestion intelligente du throttling avec backoff exponentiel
  - Rotation d'agents utilisateurs et proxies
  - Modulation dynamique des taux de requÃªtes

### âš™ï¸ Traitement et pipeline
- **Orchestration via Apache Airflow**: Gestion complÃ¨te du workflow avec DAGs configurables
- **ParallÃ©lisation optimisÃ©e**: Utilisation de ThreadPoolExecutor pour maximiser l'efficacitÃ©
- **Exportation multi-formats**: Support des formats CSV et JSON avec validation structurelle
- **ObservabilitÃ© intÃ©grÃ©e**: SystÃ¨me complet de mÃ©triques, logs structurÃ©s et rapports d'exÃ©cution

### ðŸ“Š Analyse de donnÃ©es via Snowflake
- **Pipeline ETL**: SystÃ¨me d'ingestion et transformation automatisÃ©
- **ModÃ©lisation analytique**: Tables normalisÃ©es et dÃ©normalisÃ©es pour diffÃ©rents cas d'usage
- **CapacitÃ©s analytiques**: Segmentation produit, analyse de sentiment et autres fonctionnalitÃ©s

## ðŸ“‹ PrÃ©requis

- Python 3.8+
- Apache Airflow 2.5.0+
- PostgreSQL 13+ (recommandÃ© pour Airflow)
- Compte Snowflake avec permissions adÃ©quates
- *(Optionnel)* AWS S3 pour le stockage intermÃ©diaire des donnÃ©es extraites

### DÃ©pendances Python

```
apache-airflow>=2.5.0
requests>=2.28.0
beautifulsoup4>=4.11.0
pandas>=1.5.0
selenium>=4.7.0
webdriver-manager>=3.8.0
python-dotenv>=0.21.0
snowflake-connector-python>=2.7.0
```

## ðŸ”§ Installation

```bash
# Clonage du rÃ©fÃ©rentiel
git clone https://github.com/zakariaeyahya/amazon.git
cd amazon-scraping

# Configuration de l'environnement virtuel
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# DÃ©finition du rÃ©pertoire home d'Airflow
export AIRFLOW_HOME=/path/to/airflow

# Initialisation de la base de donnÃ©es
airflow db init

# CrÃ©ation de l'utilisateur administrateur
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com \
    --password your_password

# Configuration des rÃ©pertoires nÃ©cessaires
mkdir -p $AIRFLOW_HOME/{dags,logs,plugins}
```

## ðŸ—ï¸ Architecture

```
amazon-analytics/
â”œâ”€â”€ airflow_dags/
â”‚   â””â”€â”€ amazon_scraping_dag.py
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ categories.py
â”‚   â”œâ”€â”€ amazon_details_scraper.py
â”‚   â”œâ”€â”€ commentaire.py
â”‚   â””â”€â”€ scraping.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ file_utils.py
â”‚   â”œâ”€â”€ scraping_logger.py
â”‚   â””â”€â”€ scraping_metrics.py
â”œâ”€â”€ snowflake/
â”‚   â”œâ”€â”€ SQLrequet.sql
â”‚   â””â”€â”€ snowflakeconnection.py
â”œâ”€â”€ data/
â”œâ”€â”€ logs/
â”œâ”€â”€ config/
â”œâ”€â”€ tests/
â””â”€â”€ requirements.txt
```

## âš™ï¸ Configuration

### Configuration de l'extracteur

```json
{
  "extraction": {
    "categories": ["electronics", "computers", "smart-home"],
    "limites": {
      "produits_par_categorie": 100,
      "pages_par_categorie": 5
    },
    "controle_debit": {
      "requetes_par_seconde": 2,
      "taille_burst": 5,
      "delai_apres_echec": 300
    },
    "proxies": {
      "actifs": true,
      "liste": [
        "http://proxy1.example.com:8080",
        "http://proxy2.example.com:8080"
      ],
      "intervalle_rotation": 100
    },
    "agents_utilisateurs": {
      "actifs": true,
      "intervalle_rotation": 50
    }
  },
  "traitement": {
    "taille_lot": 5,
    "travailleurs_max": 2
  }
}
```

### Configuration Snowflake

```python
# snowflakeconnection.py
conn_config = {
    'user': 'SNOWFLAKE_USER',
    'password': 'SNOWFLAKE_PASSWORD',
    'account': 'SNOWFLAKE_ACCOUNT',
    'warehouse': 'COMPUTE_WH',
    'database': 'AMAZON_ANALYTICS',
    'schema': 'PUBLIC'
}
```

## ðŸ”„ Pipeline d'exÃ©cution

### Phase d'extraction
1. VÃ©rification prÃ©liminaire - Validation de l'environnement et des dÃ©pendances
2. Extraction des catÃ©gories - Cartographie de la taxonomie produit
3. Extraction des dÃ©tails produits - Collecte des mÃ©tadonnÃ©es principales
4. Extraction des avis - Collecte des donnÃ©es utilisateurs
5. Post-traitement - Nettoyage et normalisation
6. GÃ©nÃ©ration de rapports - MÃ©triques d'exÃ©cution et validations

### Phase d'analyse Snowflake
1. Chargement des donnÃ©es - Ingestion dans les tables Snowflake
2. Transformation - Normalisation et enrichissement
3. ModÃ©lisation - CrÃ©ation des vues analytiques
4. Segmentation - Classification par prix et autres dimensions
5. Analyse avancÃ©e - RequÃªtes analytiques et rapports

## ðŸ’¾ ModÃ¨le de donnÃ©es

### Tables principales

| Table | Description |
|-------|-------------|
| `product_info1` | DonnÃ©es fondamentales produit |
| `product_info2` | SpÃ©cifications techniques produit |
| `review` | Avis et Ã©valuations utilisateurs |
| `full_product_info` | Vue dÃ©normalisÃ©e intÃ©grÃ©e |
| `classe_prix` | Segmentation par gamme de prix |

> **Note:** Le schÃ©ma est conÃ§u pour optimiser Ã  la fois les requÃªtes analytiques et les opÃ©rations OLTP.

## ðŸ’¡ Exemples d'utilisation

### Classification de produits et analyse de sentiment

```sql
SELECT
    p.asin,
    p.titre,
    p.prix,
    c.classe AS gamme_prix,
    r.sentiment,
    COUNT(r.review_id) AS nombre_avis
FROM
    full_product_info p
JOIN
    classe_prix c ON p.asin = c.asin
LEFT JOIN
    review r ON p.asin = r.asin
GROUP BY
    1, 2, 3, 4, 5
ORDER BY
    nombre_avis DESC;
```

### Transformation des spÃ©cifications techniques

```sql
-- Normalisation des tailles de disque dur en GB
UPDATE product_info2
SET disque_dur =
    CASE
        WHEN disque_dur LIKE '%TB%' THEN TO_NUMBER(REGEXP_SUBSTR(disque_dur, '[0-9]+')) * 1024
        WHEN disque_dur LIKE '%GB%' THEN TO_NUMBER(REGEXP_SUBSTR(disque_dur, '[0-9]+'))
        ELSE NULL
    END;
```

## ðŸ”§ RÃ©solution des problÃ¨mes

### DAGs Airflow
- VÃ©rifier les permissions des fichiers DAG
- Valider la syntaxe Python avec `airflow dags list` et `airflow dags show`
- Consulter les logs pour erreurs de syntaxe ou d'importation

### Limitations de taux
- ImplÃ©menter une stratÃ©gie de backoff progressif
- Augmenter l'intervalle entre requÃªtes
- Diversifier les proxies et agents utilisateurs

### Connexion Snowflake
- VÃ©rifier les identifiants et permissions
- S'assurer que l'adresse IP est autorisÃ©e
- Tester la connexion avec une requÃªte simple

## ðŸ“œ Licence

Ce projet est distribuÃ© sous [licence MIT](LICENSE).

---

Â© 2025 - Plateforme d'Extraction et d'Analyse de Produits Amazon - Tous droits rÃ©servÃ©s
